{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e785bea-fbe9-4d98-9be7-d8a5186364a7",
   "metadata": {},
   "source": [
    "# Hi!\n",
    "##### A little bit of context: I was asked if it was possible to get Google Trend's \"interest\" values for different companies across a period of time. While googling for a way to do this, I came across the \"pytrends\" library (https://pypi.org/project/pytrends/) which allows you to pull data from Google Trends using Python, with certain conditions of course. \n",
    "\n",
    "##### A main limitation in our task at hand, was that the list of keywords meant to look for on Google Trends was too big, as you can only provide 5 keywords per search. Not only this, the \"interest scores\" provided are regularized values in the context of the keywords that you provide, ranging from 0 to 100. This means that the resulting score for a particular keyword -could- vary according to the different combination of keywords provided. Therefore, a main osbtacle was finding a workaround for a list of hundreds of words.\n",
    "\n",
    "##### Through several trial and errors trying to understand how the different results could vary, I noticed that we could achieve a certain degree of consistency if we searched for a term that had relatively high scores, as it would serve as a \"reference\" in the scaling of the other scores for the remaining terms, kind of like an anchor. In our particular case (based on my trial and errors) I noticed that a term that was giving consistent scores for the other terms in the keyword list was \"Facebook\", so I decided to have it consistently in my keywords list as the \"score anchor\". The remaining challenge was to loop the keywords and assemble a hierarchy of companies based on their average interest score. The following code aims to solve this situation.\n",
    "\n",
    "##### Please keep in mind that we can only make a certain amount of requests continously with pytrends, as with an excessive amount we would start looking like a bot and get temporarily blocked with a \"429 error\". So if your list contains a lot of elements as well, it is likely that you may want to split your data, or break the process in different steps on different occassions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830915df-f5f1-42f8-af6d-8db09aff55db",
   "metadata": {},
   "source": [
    "# Alright then! \n",
    "\n",
    "##### The first thing we will do is install pytrends and run a sample request using the \"TrendReq()\" function. We will then run an \"interest over time\" request for our desired keyword list.\n",
    "\n",
    "###### Please remember that you may find more details regarding pytrends here: https://pypi.org/project/pytrends/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23aece7f-aabe-43c8-b55a-39c4fb582ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytrends in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (4.9.0)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from pytrends) (2.25.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from pytrends) (4.6.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from pytrends) (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (1.20.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jpmli\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (2.10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Amazon</th>\n",
       "      <th>Netflix</th>\n",
       "      <th>Google</th>\n",
       "      <th>isPartial</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>96</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>96</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-25</th>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-29</th>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-05</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-12</th>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-19</th>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-26</th>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Facebook  Apple  Amazon  Netflix  Google  isPartial\n",
       "date                                                           \n",
       "2018-03-04        96      6      18        7      68      False\n",
       "2018-03-11       100      6      18        7      68      False\n",
       "2018-03-18        96      6      18        7      69      False\n",
       "2018-03-25        99      7      17        8      64      False\n",
       "2018-04-01        94      6      18        8      64      False\n",
       "...              ...    ...     ...      ...     ...        ...\n",
       "2023-01-29        31      7      20        9      57      False\n",
       "2023-02-05        30      7      19        9      54      False\n",
       "2023-02-12        32      7      19        8      55      False\n",
       "2023-02-19        32      7      19        9      53      False\n",
       "2023-02-26        34      7      20        8      56       True\n",
       "\n",
       "[261 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install pytrends\n",
    "!pip install pytrends\n",
    "#import the libraries\n",
    "import pandas as pd                        \n",
    "from pytrends.request import TrendReq\n",
    "pytrend = TrendReq()\n",
    "#provide your search terms\n",
    "kw_list=['Facebook', 'Apple', 'Amazon', 'Netflix', 'Google']\n",
    "#get interest by region for your search terms\n",
    "pytrend.build_payload(kw_list=kw_list)\n",
    "df = pytrend.interest_over_time()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71173c7-e562-4277-b1b8-965b7fdcd7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elements of the list are:  ['Nike', 'Google', 'Apple', 'Amazon', 'Microsoft', 'Netflix', 'Nestle']\n",
      "The grupos object consists of:  [['Facebook', 'Nike', 'Google', 'Apple'], ['Facebook', 'Amazon', 'Microsoft', 'Netflix'], ['Facebook', 'Nestle']]\n"
     ]
    }
   ],
   "source": [
    "#Now let's ingest the company list into the notebook. I purposely created a much shorter version in the \"small sample\" sheet for illustration purposes.\n",
    "df = pd.read_excel('listado_empresas.xlsx', sheet_name='small_sample')\n",
    "\n",
    "#Let's just call the list object as its Spanish translation \"lista\", quite creative I am aware\n",
    "lista = df.Nombre.tolist()\n",
    "\n",
    "#Checking the content of \"lista\"\n",
    "print(\"The elements of the list are: \", lista)\n",
    "\n",
    "#Getting the length of the \"lista\" object, as it will be useful for looping later.\n",
    "lista_len = len(lista)\n",
    "\n",
    "#Now we are going to define a function called \"chunks\" which will help us split the list content into sub-chunks of lists in order to be able \n",
    "#to iterate over them.\n",
    "#Disclaimer: The remaining code for this cell is not of my creationg, I found it here when I got stuck at the start: \n",
    "#https://github.com/GeneralMills/pytrends/issues/485\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), 3):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "#Now let's create a \"grupos\" object (Spanish for groups, I know I know) which will make a big list formed of all the small lists from \"chunks\".\n",
    "#For each of the sublists, we will be adding \"Facebook\", as it will help us gain consistency in the scores.\n",
    "grupos = [ ['Facebook'] + lst  for lst in list(chunks(lista, 3))]\n",
    "print(\"The grupos object consists of: \", grupos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f598bce-4243-4852-9ec2-90eca534ff3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8e2f6d-06a6-497e-b264-fa38badbba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's create a DataFrame containing only the scores for Facebook. We will call this our \"main dataframe\" as it will help us anchor the other results.  \n",
    "#We are using parameters on TrendReq to adjust the search of interest inside the United States with a time zone of CST.\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "#Our only keyword will be \"Facebook\" for the moment.\n",
    "kw_list = ['Facebook']\n",
    "#We are passing our keyword list as well as our category number corresponding for \"Business Finance\" (https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories)\n",
    "#We are also pulling scores from the last 5 years.\n",
    "pytrends.build_payload(kw_list, cat=112, timeframe='today 5-y')\n",
    "main_df = pytrends.interest_over_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e343b0d-ed05-400c-bb61-81d043bf1981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facebook</th>\n",
       "      <th>isPartial</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-04</th>\n",
       "      <td>71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-11</th>\n",
       "      <td>79</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>68</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-25</th>\n",
       "      <td>77</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-29</th>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-05</th>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-12</th>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-19</th>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-26</th>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Facebook  isPartial\n",
       "date                           \n",
       "2018-03-04        71      False\n",
       "2018-03-11        79      False\n",
       "2018-03-18        68      False\n",
       "2018-03-25        77      False\n",
       "2018-04-01        74      False\n",
       "...              ...        ...\n",
       "2023-01-29        35      False\n",
       "2023-02-05        40      False\n",
       "2023-02-12        40      False\n",
       "2023-02-19        41      False\n",
       "2023-02-26        38       True\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61df26b0-65db-4ca3-acba-9d1439df68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also remove the \"isPartial\" column\n",
    "main_df.drop('isPartial', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c9e49c-c60e-43cc-926b-7f3c9af20264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_result:  Nike\n",
      "hierarchy pre append:  ['Facebook']\n",
      "hierarchy post append:  ['Facebook', 'Nike']\n",
      "max_result:  Google\n",
      "hierarchy pre append:  ['Facebook', 'Nike']\n",
      "hierarchy post append:  ['Facebook', 'Nike', 'Google']\n",
      "max_result:  Amazon\n",
      "hierarchy pre append:  ['Facebook', 'Nike', 'Google']\n",
      "hierarchy post append:  ['Facebook', 'Nike', 'Google', 'Amazon']\n",
      "max_result:  Netflix\n",
      "hierarchy pre append:  ['Facebook', 'Nike', 'Google', 'Amazon']\n",
      "hierarchy post append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix']\n",
      "max_result:  Apple\n",
      "hierarchy pre append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix']\n",
      "hierarchy post append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix', 'Apple']\n",
      "max_result:  Microsoft\n",
      "hierarchy pre append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix', 'Apple']\n",
      "hierarchy post append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix', 'Apple', 'Microsoft']\n",
      "max_result:  Nestle\n",
      "hierarchy pre append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix', 'Apple', 'Microsoft']\n",
      "hierarchy post append:  ['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix', 'Apple', 'Microsoft', 'Nestle']\n"
     ]
    }
   ],
   "source": [
    "#We begin by creating a list called \"hierarchy\" which will start with \"Facebook\", as we are assuming it to be our \"anchor\" value that is higher than \n",
    "#the rest.\n",
    "hierarchy = ['Facebook']\n",
    "\n",
    "#Now we will iterate over the amount of elements in the list. As we will start comparing the scores of each of them, get the highest one, add it to \n",
    "#the hierarchy list, and then run again with the remaining elements. Thus making sure we are creating a list of the companies with the highest average\n",
    "#interest score. Notice how this isn't necessary in this small sample case, but if you wish to solve this situation for a big amount of terms, you \n",
    "#will probably have to split the list and to this on different occassions, as Google can only provide you with a certain amount of requests and it \n",
    "#could very well be impossible to finish this in one go for different elements.\n",
    "for l in range(lista_len):\n",
    "    #we need to run the grupos object inside of the loop, as we will be updating the list elements, and therefore grupos too.\n",
    "    grupos = [ ['Facebook'] + lst  for lst in list(chunks(lista, 3))]\n",
    "    #getting each individual sub list from grupos...\n",
    "    for g in grupos:\n",
    "        #passing the sublist as a keyword element...\n",
    "        kw_list = g\n",
    "        pytrends.build_payload(kw_list, cat=112, timeframe='today 5-y')\n",
    "        #getting dataframe with interest scores over time\n",
    "        df = pytrends.interest_over_time()\n",
    "        #now we are adding each element from the dataframe into our original main_df\n",
    "        for elemento in range(len(g)):\n",
    "            main_df[g[elemento]] = df.iloc[:,elemento]\n",
    "            #print(\"main_df: \",main_df.head(2))\n",
    "    #run this in case you want to check over \"g\"\n",
    "    #print(\"g: \", g)\n",
    "    #now we are getting the average values of the interest scores for each column (therefore each element)\n",
    "    #we sort them in descending order so we can pick our elements with the highest averages for the hierarchy\n",
    "    #we also pick the second largest, as the first one will always be \"Facebook\"\n",
    "    max_result = main_df.mean().sort_values(ascending=False).index[1]\n",
    "    #Just if you want to check how the values behave...\n",
    "    print(\"max_result: \", max_result)\n",
    "    print(\"hierarchy pre append: \", hierarchy)\n",
    "    #We append the max value that we gor into the list\n",
    "    hierarchy.append(max_result)\n",
    "    print(\"hierarchy post append: \", hierarchy)\n",
    "    #and remove it from the original list, so we can start again without our recently appended value and look for the next one in the hierarchy\n",
    "    lista.remove(max_result)\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)\n",
    "    #now we need to \"reset\" main_df so we can go another round with our updated list and keep fulling the hierarchy\n",
    "    kw_list = ['Facebook']\n",
    "    pytrends.build_payload(kw_list, cat=112, timeframe='today 5-y')\n",
    "    main_df = pytrends.interest_over_time()\n",
    "    main_df.drop('isPartial', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796c63ae-6340-4850-a592-1268f38d444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'Nike', 'Google', 'Amazon', 'Netflix', 'Apple', 'Microsoft', 'Nestle']\n"
     ]
    }
   ],
   "source": [
    "#And finally we can check on hour hierarchy list of average intensity scores for our desired keyword list!\n",
    "print(hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab67a6-36cc-41ce-90d7-ab25eda4ba81",
   "metadata": {},
   "source": [
    "##### I am aware that this may be overkill for our small sample, but in the case that you have a huge amount of elements and get blocked, it would be useful to run this code on small iterations (changing the <<range(lista_len)>> part at the start of the loop for small and increasing values) so that you can still get the hierarchy even if you run the code on different occations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2890c-e1f3-4e9f-8beb-dc1cff6450cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
